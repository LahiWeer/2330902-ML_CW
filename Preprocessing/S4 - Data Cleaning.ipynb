{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/LahiWeer/2330902-ML_CW/blob/master/Preprocessing/S4%20-%20Data%20Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "4978dcc6c8194309"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE"
   ],
   "metadata": {
    "id": "k-i76H2gp_7U",
    "ExecuteTime": {
     "end_time": "2024-12-25T09:09:40.761292Z",
     "start_time": "2024-12-25T09:09:39.109325Z"
    }
   },
   "id": "k-i76H2gp_7U",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "id": "initial_id",
    "outputId": "9302a745-4203-49e7-a3d4-a21991652998",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-12-25T09:03:13.306407Z",
     "start_time": "2024-12-25T09:03:10.342005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_url = 'https://raw.githubusercontent.com/LahiWeer/2330902-ML_CW/refs/heads/master/Preprocessing/Processed%20data%20in%20each%20step/processed_bank_data_final.csv'\n",
    "df = pd.read_csv(data_url)\n",
    "\n",
    "print(df['y'].value_counts())\n",
    "print(f\"\\nTotal rows in the original DataFrame: {len(df)}\")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "0    36193\n",
      "1     4594\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total rows in the original DataFrame: 40787\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "id": "6a15e6f85f863125",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-25T09:07:23.710384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Find duplicate rows and mark the first occurrence index for duplicates\n",
    "duplicates = df[df.duplicated(keep=False)]  # Get all duplicated rows\n",
    "\n",
    "# Add a new column 'is_duplicate' and 'duplicate_with' to highlight duplicates and show their first occurrence\n",
    "df['is_duplicate'] = ''\n",
    "df['duplicate_with'] = ''\n",
    "\n",
    "# Get the index of the first occurrence of each duplicate\n",
    "first_occurrence_index = df[df.duplicated(keep='first')].index\n",
    "\n",
    "# Loop through the DataFrame to mark duplicates and their first occurrence\n",
    "for idx in first_occurrence_index:\n",
    "    # Find the first occurrence index\n",
    "    first_occurrence = df[df.iloc[:, :].eq(df.iloc[idx, :]).all(axis=1)].index[0]\n",
    "\n",
    "    # Mark the duplicate row and the first occurrence\n",
    "    df.at[idx, 'is_duplicate'] = 'Duplicate'\n",
    "    df.at[idx, 'duplicate_with'] = f\"Row {first_occurrence}\""
   ],
   "id": "6a15e6f85f863125",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T14:32:40.076726Z",
     "start_time": "2024-12-22T14:32:40.065098Z"
    },
    "id": "b0aab2476e924ad6",
    "outputId": "02b06ad8-1d73-4d10-e1f1-ba0d0d932f30",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "DataFrame with 'is_duplicate' and 'duplicate_with' columns:\n",
      "   age  job_blue-collar  job_entrepreneur  job_housemaid  job_management  \\\n",
      "0   56                0                 0              1               0   \n",
      "1   57                0                 0              0               0   \n",
      "2   37                0                 0              0               0   \n",
      "3   40                0                 0              0               0   \n",
      "4   56                0                 0              0               0   \n",
      "\n",
      "   job_retired  job_self-employed  job_services  job_student  job_technician  \\\n",
      "0            0                  0             0            0               0   \n",
      "1            0                  0             1            0               0   \n",
      "2            0                  0             1            0               0   \n",
      "3            0                  0             0            0               0   \n",
      "4            0                  0             1            0               0   \n",
      "\n",
      "   ...  previous  poutcome  emp.var.rate  cons.price.idx  cons.conf.idx  \\\n",
      "0  ...         0        -1           1.1          93.994          -36.4   \n",
      "1  ...         0        -1           1.1          93.994          -36.4   \n",
      "2  ...         0        -1           1.1          93.994          -36.4   \n",
      "3  ...         0        -1           1.1          93.994          -36.4   \n",
      "4  ...         0        -1           1.1          93.994          -36.4   \n",
      "\n",
      "   euribor3m  nr.employed  y  is_duplicate  duplicate_with  \n",
      "0      4.857       5191.0  0                                \n",
      "1      4.857       5191.0  0                                \n",
      "2      4.857       5191.0  0                                \n",
      "3      4.857       5191.0  0                                \n",
      "4      4.857       5191.0  0                                \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "execution_count": 19,
   "source": [
    "# Display the DataFrame with the new 'is_duplicate' and 'duplicate_with' columns\n",
    "print(\"\\nDataFrame with 'is_duplicate' and 'duplicate_with' columns:\")\n",
    "print(df.head())  # Display the first few rows of the updated DataFrame"
   ],
   "id": "b0aab2476e924ad6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T14:32:40.134848Z",
     "start_time": "2024-12-22T14:32:40.104309Z"
    },
    "id": "e2792885ed29458d",
    "outputId": "b9860d9e-fded-4a25-abaf-99daf61e744d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Number of duplicate rows: 277\n"
     ]
    }
   ],
   "execution_count": 20,
   "source": [
    "# Count duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicate_count}\")"
   ],
   "id": "e2792885ed29458d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T14:32:40.428523Z",
     "start_time": "2024-12-22T14:32:40.149651Z"
    },
    "id": "2b820b39ee611eb0",
    "outputId": "df9598c8-39ff-4ebd-9069-4dc4b1b0b440",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Updated DataFrame saved to: highlighted_duplicates_with_row_numbers.csv\n"
     ]
    }
   ],
   "execution_count": 11,
   "source": [
    "# Save the updated DataFrame with the new columns to a CSV file\n",
    "output_file_path = 'highlighted_duplicates_with_row_numbers.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "print(f\"\\nUpdated DataFrame saved to: {output_file_path}\")"
   ],
   "id": "2b820b39ee611eb0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T14:32:40.686378Z",
     "start_time": "2024-12-22T14:32:40.442820Z"
    },
    "id": "77ad6b1449053747",
    "outputId": "426328d6-9514-47b2-d296-b56ab9bc04dc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Total number of rows after removing duplicates: 38931\n",
      "\n",
      "Filtered DataFrame (without duplicates) saved to: cleaned_data_without_duplicates.csv\n"
     ]
    }
   ],
   "execution_count": 21,
   "source": [
    "# Remove the rows that are marked as duplicates\n",
    "df_filtered = df[df['is_duplicate'] != 'Duplicate']\n",
    "\n",
    "# Count the total number of rows after removing duplicates\n",
    "total_rows_after_removal = df_filtered.shape[0]\n",
    "print(f\"\\nTotal number of rows after removing duplicates: {total_rows_after_removal}\")\n",
    "\n",
    "# Save the filtered DataFrame (without duplicates) to the original file or a new file\n",
    "filtered_file_path = 'cleaned_data_without_duplicates.csv'\n",
    "df_filtered.to_csv(filtered_file_path, index=False)\n",
    "print(f\"\\nFiltered DataFrame (without duplicates) saved to: {filtered_file_path}\")"
   ],
   "id": "77ad6b1449053747"
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the uploaded CSV file to inspect its structure\n",
    "bank_data_url= 'https://raw.githubusercontent.com/LahiWeer/2330902-ML_CW/refs/heads/master/Preprocessing/Processed%20data%20in%20each%20step/cleaned_data_without_duplicates.csv'\n",
    "bank_data = pd.read_csv(bank_data_url)"
   ],
   "metadata": {
    "id": "Pc3l60oktm2l"
   },
   "id": "Pc3l60oktm2l",
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Print the initial row count\n",
    "initial_row_count = len(bank_data)\n",
    "print(f\"Initial row count: {initial_row_count}\")"
   ],
   "metadata": {
    "id": "tvXVjm3pwWxJ",
    "outputId": "9a34aa9f-9fe9-44df-fc0f-4e2d8a8a2635",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "tvXVjm3pwWxJ",
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial row count: 38931\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Preprocess the data\n",
    "# Filter invalid rows where pdays = 999 and poutcome != -1\n",
    "invalid_rows = bank_data[(bank_data['pdays'] == 999) & (bank_data['poutcome'] != -1)]\n",
    "invalid_count = len(invalid_rows)\n",
    "total_rows = len(bank_data)\n",
    "removed_percentage = (invalid_count / total_rows) * 100"
   ],
   "metadata": {
    "id": "U2tnh2zowbHt"
   },
   "id": "U2tnh2zowbHt",
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the invalid rows to a separate CSV file\n",
    "invalid_rows.to_csv('invalid_rows_pdays_999.csv', index=False)\n",
    "print(f\"Saved {invalid_count} invalid rows to 'invalid_rows_pdays_999.csv'.\")"
   ],
   "metadata": {
    "id": "GbADtQzzwj0d",
    "outputId": "dde9b455-fbe4-4f6e-fa5d-ac14f70b7ab3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "GbADtQzzwj0d",
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved 4001 invalid rows to 'invalid_rows_pdays_999.csv'.\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:02:06.144494Z",
     "start_time": "2024-12-23T12:01:58.237435Z"
    },
    "id": "381961d4906e753",
    "outputId": "08fbbd75-27c0-464d-b310-249a2d23e805",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Row count of cleaned dataset: 34930\n",
      "Filtered 4001 invalid rows, which is 10.28% of the total dataset.\n"
     ]
    }
   ],
   "execution_count": 26,
   "source": [
    "# Create a cleaned version of the dataset without altering the original dataset\n",
    "bank_data_cleaned = bank_data[~bank_data.index.isin(invalid_rows.index)]\n",
    "\n",
    "# Print the updated row count for the cleaned dataset\n",
    "final_row_count = len(bank_data_cleaned)\n",
    "print(f\"Row count of cleaned dataset: {final_row_count}\")\n",
    "\n",
    "# Summarize the percentage of invalid rows filtered\n",
    "print(f\"Filtered {invalid_count} invalid rows, which is {removed_percentage:.2f}% of the total dataset.\")"
   ],
   "id": "381961d4906e753"
  },
  {
   "metadata": {
    "id": "931d9c6e4b4e9629",
    "outputId": "f1d1f643-53a9-4018-b8d4-0da8839b915f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved the cleaned dataset to 'bank_data_cleaned_final.csv'.\n"
     ]
    }
   ],
   "execution_count": 28,
   "source": [
    "# Save the cleaned dataset to a separate CSV file\n",
    "bank_data_cleaned.to_csv('bank_data_cleaned_final.csv', index=False)\n",
    "print(\"Saved the cleaned dataset to 'bank_data_cleaned_final.csv'.\")"
   ],
   "id": "931d9c6e4b4e9629"
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "df_url = 'https://raw.githubusercontent.com/LahiWeer/2330902-ML_CW/refs/heads/master/Preprocessing/Processed%20data%20in%20each%20step/bank_data_cleaned_final.csv'\n",
    "df= pd.read_csv(df_url)"
   ],
   "metadata": {
    "id": "0RLjZ6YYyCqW"
   },
   "id": "0RLjZ6YYyCqW",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. Keep a copy of the original dataset\n",
    "original_df = df.copy()\n",
    "\n",
    "# 2. Detect outliers in the 'campaign' column (campaign >= 17)\n",
    "outliers = df[df['campaign'] > 17]\n",
    "\n",
    "# 3. Remove outliers and create a new dataset\n",
    "df_no_outliers = df[df['campaign'] <= 17]\n",
    "df_no_outliers.to_csv('Updated_cleaned_data_no_outliers.csv', index=False)\n",
    "\n",
    "# Notify the user\n",
    "print(\"Outliers have been removed and the cleaned dataset is saved as 'Updated_cleaned_data_no_outliers.csv'.\")"
   ],
   "metadata": {
    "id": "gDnFttBGzVhv",
    "outputId": "50b2c412-7673-4f7d-a2f1-bbacddd0cabe",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "gDnFttBGzVhv",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Outliers have been removed and the cleaned dataset is saved as 'Updated_cleaned_data_no_outliers.csv'.\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "id": "a9d2a4393e663764",
    "outputId": "bbe7ef88-4578-4934-9c59-888a25053737",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "ExecuteTime": {
     "end_time": "2024-12-25T09:04:43.022932Z",
     "start_time": "2024-12-25T09:04:34.569300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def balance_dataset_with_smote(original_file, target_column, output_file):\n",
    "    # Step 1: Load the Original Dataset\n",
    "    data = pd.read_csv(original_file)\n",
    "    X = data.drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "\n",
    "    # Step 2: Analyze Class Distribution\n",
    "    class_counts = y.value_counts()\n",
    "    count_y0, count_y1 = class_counts[0], class_counts[1]\n",
    "    rows_to_add_y1 = count_y0 - count_y1\n",
    "\n",
    "    # Step 3: Check if Resampling is Required\n",
    "    if rows_to_add_y1 <= 0:\n",
    "        print(\"Dataset is already balanced.\")\n",
    "        data.to_csv(output_file, index=False)\n",
    "        return\n",
    "\n",
    "    # Step 4: Reapply SMOTE to Generate Synthetic Data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    resampled_data[target_column] = y_resampled\n",
    "\n",
    "    # Save the original data\n",
    "    original_data = data.copy()\n",
    "    original_data.to_csv(output_file, index=False)\n",
    "\n",
    "    # Step 5: Filter and Prepare the Minority Class Rows\n",
    "    resampled_data_y1 = resampled_data[resampled_data[target_column] == 1]\n",
    "\n",
    "    # Step 6: Check for Duplicates Before Appending\n",
    "    resampled_and_original_data = pd.read_csv(output_file)\n",
    "\n",
    "    # Remove duplicates using concat and drop_duplicates\n",
    "    combined_data = pd.concat([resampled_and_original_data, resampled_data_y1])\n",
    "    combined_data = combined_data.drop_duplicates(keep=False)\n",
    "\n",
    "    # Append only the required number of non-duplicate rows\n",
    "    resampled_data_y1_to_append = combined_data[combined_data[target_column] == 1].head(rows_to_add_y1)\n",
    "    resampled_and_original_data = pd.concat(\n",
    "        [resampled_and_original_data, resampled_data_y1_to_append], ignore_index=True\n",
    "    )\n",
    "    resampled_and_original_data.to_csv(output_file, index=False)\n",
    "\n",
    "    # Step 7: Check Final Balance and Reapply SMOTE if Necessary\n",
    "    while True:\n",
    "        final_class_counts = resampled_and_original_data[target_column].value_counts()\n",
    "        count_y0, count_y1 = final_class_counts[0], final_class_counts[1]\n",
    "\n",
    "        if count_y0 == count_y1:\n",
    "            break  # Dataset is balanced; no further action required.\n",
    "\n",
    "        # Reapply SMOTE on the combined dataset\n",
    "        X = resampled_and_original_data.drop(columns=[target_column])\n",
    "        y = resampled_and_original_data[target_column]\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "        resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "        resampled_data[target_column] = y_resampled\n",
    "\n",
    "        # Filter the newly generated y == 1 rows\n",
    "        resampled_data_y1 = resampled_data[resampled_data[target_column] == 1]\n",
    "\n",
    "        # Remove duplicates using concat and drop_duplicates\n",
    "        combined_data = pd.concat([resampled_and_original_data, resampled_data_y1])\n",
    "        combined_data = combined_data.drop_duplicates(keep=False)\n",
    "\n",
    "        # Append only the required number of non-duplicate rows\n",
    "        rows_to_add_y1 = count_y0 - count_y1\n",
    "        resampled_data_y1_to_append = combined_data[combined_data[target_column] == 1].head(rows_to_add_y1)\n",
    "        resampled_and_original_data = pd.concat(\n",
    "            [resampled_and_original_data, resampled_data_y1_to_append], ignore_index=True\n",
    "        )\n",
    "        resampled_and_original_data.to_csv(output_file, index=False)\n",
    "\n",
    "    # Step 8: Verify Final Class Distribution\n",
    "    final_class_counts = resampled_and_original_data[target_column].value_counts()\n",
    "    print(f\"Final Class Distribution:\\n{final_class_counts}\")\n",
    "    print(f\"Balanced dataset saved to '{output_file}'.\")\n",
    "\n",
    "# Input file paths and target column\n",
    "original_file = \"https://raw.githubusercontent.com/LahiWeer/2330902-ML_CW/refs/heads/master/Preprocessing/Processed%20data%20in%20each%20step/Updated_cleaned_data_no_outliers.csv\"\n",
    "output_file = \"resampled_and_original_data.csv\"\n",
    "target_column = \"y\"\n",
    "\n",
    "# Balance the dataset\n",
    "balance_dataset_with_smote(original_file, target_column, output_file)"
   ],
   "id": "a9d2a4393e663764",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Class Distribution:\n",
      "y\n",
      "0    30659\n",
      "1    30659\n",
      "Name: count, dtype: int64\n",
      "Balanced dataset saved to 'resampled_and_original_data.csv'.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "id": "d07afb572aa10f60",
    "outputId": "f3bbab6c-22f7-4625-8334-6c708479b08b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "ExecuteTime": {
     "end_time": "2024-12-25T09:07:05.656083Z",
     "start_time": "2024-12-25T09:07:04.487601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "check_equal_data = pd.read_csv(\"https://raw.githubusercontent.com/LahiWeer/2330902-ML_CW/refs/heads/master/Preprocessing/resampled_and_original_data.csv\")\n",
    "check_equal_data['y'].value_counts()"
   ],
   "id": "d07afb572aa10f60",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "0    30659\n",
       "1    30659\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T07:38:35.318598Z",
     "start_time": "2024-12-24T07:38:35.157455Z"
    },
    "id": "ca170ead3ca0fe7e",
    "outputId": "072f6be7-bdb6-4732-ccc4-c9a0cc48d8f6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "execution_count": 37,
   "source": [
    "# Find duplicate rows\n",
    "duplicates = check_equal_data[check_equal_data.duplicated(keep=False)]  # Get all duplicated rows\n",
    "\n",
    "# Count duplicates\n",
    "duplicate_count = check_equal_data.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicate_count}\")"
   ],
   "id": "ca170ead3ca0fe7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T17:36:50.940318Z",
     "start_time": "2024-12-23T17:36:49.475330Z"
    },
    "id": "1717d1b13c8e87a",
    "outputId": "b02c3b20-2b20-4093-a01d-2c2e1acea0c8"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled and original data saved to 'resampled_and_original_data.csv'\n"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "# import pandas as pd\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# # Example: Load your dataset\n",
    "# data = pd.read_csv(\"Updated_cleaned_data_no_outliers.csv\")\n",
    "#\n",
    "# # Split features and target\n",
    "# X = data.drop(columns=[\"y\"])  # Features\n",
    "# y = data[\"y\"]                 # Target\n",
    "#\n",
    "# # Apply train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "#\n",
    "# # Visualize original class distribution\n",
    "# original_counts = y_train.value_counts()\n",
    "#\n",
    "# # Apply SMOTE\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "#\n",
    "# # Convert resampled data to DataFrame\n",
    "# X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "# y_resampled_df = pd.DataFrame(y_resampled, columns=[\"y\"])\n",
    "#\n",
    "# # Combine features and target for resampled data\n",
    "# resampled_data = pd.concat([X_resampled_df, y_resampled_df], axis=1)\n",
    "#\n",
    "# # Filter the rows where y == 1\n",
    "# resampled_data_y1 = resampled_data[resampled_data[\"y\"] == 1]\n",
    "#\n",
    "# # Calculate the number of y == 1 rows to append to make counts equal\n",
    "# count_y0 = original_counts[0]  # Count of y == 0 in original data\n",
    "# count_y1 = original_counts[1]  # Count of y == 1 in original data\n",
    "# rows_to_add_y1 = count_y0 - count_y1  # The difference, i.e., how many y == 1 rows to append\n",
    "#\n",
    "# # Take only the necessary number of y == 1 rows from resampled data\n",
    "# resampled_data_y1_to_append = resampled_data_y1.head(rows_to_add_y1)\n",
    "#\n",
    "# # Combine original data with only the necessary y == 1 rows from the resampled data\n",
    "# original_data = pd.concat([X_train, y_train], axis=1)\n",
    "# combined_data = pd.concat([original_data, resampled_data_y1_to_append], ignore_index=True)\n",
    "#\n",
    "# # Save to CSV\n",
    "# combined_data.to_csv(\"resampled_and_original_data.csv\", index=False)\n",
    "#\n",
    "# print(\"Resampled and original data saved to 'resampled_and_original_data.csv'\")"
   ],
   "id": "1717d1b13c8e87a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T04:15:17.600888Z",
     "start_time": "2024-12-24T04:15:12.347897Z"
    },
    "id": "2b7408bbfdc5689b",
    "outputId": "2e02a59b-6d5a-4da7-a359-00a23f53d202"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Class distribution:\n",
      "y\n",
      "0    24527\n",
      "1     3222\n",
      "Name: count, dtype: int64\n",
      "Iteration 1: Class distribution:\n",
      "y\n",
      "0    24527\n",
      "1    22072\n",
      "Name: count, dtype: int64\n",
      "Iteration 2: Class distribution:\n",
      "y\n",
      "0    24527\n",
      "1    24372\n",
      "Name: count, dtype: int64\n",
      "Iteration 3: Class distribution:\n",
      "y\n",
      "0    24527\n",
      "1    24510\n",
      "Name: count, dtype: int64\n",
      "Iteration 4: Class distribution:\n",
      "y\n",
      "0    24527\n",
      "1    24522\n",
      "Name: count, dtype: int64\n",
      "Iteration 5: Class distribution:\n",
      "y\n",
      "0    24527\n",
      "1    24526\n",
      "Name: count, dtype: int64\n",
      "Iteration 6: Class distribution:\n",
      "y\n",
      "0    24527\n",
      "1    24527\n",
      "Name: count, dtype: int64\n",
      "Balanced dataset achieved.\n",
      "Final balanced dataset saved to 'resampled_and_original_data1.csv'\n",
      "Final class distribution after balancing:\n",
      " y\n",
      "0    24527\n",
      "1    24527\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "# import pandas as pd\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# # Function to resample data until balance is achieved\n",
    "# def ensure_balanced_data(data, target_column, output_file):\n",
    "#     smote = SMOTE(random_state=42)\n",
    "#     max_iterations = 10  # Limit iterations to avoid infinite loops in edge cases\n",
    "#     iteration = 0\n",
    "#\n",
    "#     while iteration < max_iterations:\n",
    "#         # Check current class distribution\n",
    "#         class_counts = data[target_column].value_counts()\n",
    "#         print(f\"Iteration {iteration}: Class distribution:\\n{class_counts}\")\n",
    "#\n",
    "#         if class_counts.min() == class_counts.max():\n",
    "#             print(\"Balanced dataset achieved.\")\n",
    "#             break\n",
    "#\n",
    "#         # Determine the majority and minority class\n",
    "#         majority_class = class_counts.idxmax()\n",
    "#         minority_class = class_counts.idxmin()\n",
    "#\n",
    "#         # Separate features and target\n",
    "#         X = data.drop(columns=[target_column])\n",
    "#         y = data[target_column]\n",
    "#\n",
    "#         # Resample to balance classes\n",
    "#         sampling_strategy = {minority_class: class_counts[majority_class]}\n",
    "#         X_resampled, y_resampled = SMOTE(random_state=42, sampling_strategy=sampling_strategy).fit_resample(X, y)\n",
    "#\n",
    "#         # Combine resampled features and target\n",
    "#         resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "#         resampled_data[target_column] = y_resampled\n",
    "#\n",
    "#         # Remove duplicates\n",
    "#         resampled_data = resampled_data.drop_duplicates(keep=\"first\")\n",
    "#\n",
    "#         # Update the dataset with resampled data\n",
    "#         data = pd.concat([data, resampled_data], ignore_index=True).drop_duplicates(keep=\"first\")\n",
    "#         iteration += 1\n",
    "#\n",
    "#     # Save the balanced dataset\n",
    "#     data.to_csv(output_file, index=False)\n",
    "#     print(f\"Final balanced dataset saved to '{output_file}'\")\n",
    "#\n",
    "#     # Verify final class distribution\n",
    "#     final_counts = data[target_column].value_counts()\n",
    "#     print(\"Final class distribution after balancing:\\n\", final_counts)\n",
    "#\n",
    "# # Load the original cleaned data\n",
    "# original_data = pd.read_csv(\"Updated_cleaned_data_no_outliers.csv\")\n",
    "#\n",
    "# # Split features and target\n",
    "# X = original_data.drop(columns=[\"y\"])  # Features\n",
    "# y = original_data[\"y\"]                 # Target\n",
    "#\n",
    "# # Apply train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "#\n",
    "# # Combine training data\n",
    "# train_data = pd.concat([X_train, y_train], axis=1)\n",
    "#\n",
    "# # Save initial combined dataset and ensure balance\n",
    "# output_file = \"resampled_and_original_data1.csv\"\n",
    "# ensure_balanced_data(train_data, target_column=\"y\", output_file=output_file)"
   ],
   "id": "2b7408bbfdc5689b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:26:00.569571Z",
     "start_time": "2024-12-23T18:25:59.763560Z"
    },
    "id": "30b0b09cfe9345ce",
    "outputId": "e9896dd8-e6d0-4640-8f45-7b43cf7573b4"
   },
   "cell_type": "code",
   "source": [
    "# import pandas as pd\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "#\n",
    "# # Step 1: Load the dataset\n",
    "# data = pd.read_csv(\"Updated_cleaned_data_no_outliers.csv\")\n",
    "#\n",
    "# # Step 2: Separate features and target\n",
    "# X = data.drop(columns=[\"y\"])  # Features\n",
    "# y = data[\"y\"]                 # Target (prediction column)\n",
    "#\n",
    "# # Step 3: Apply SMOTE\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "#\n",
    "# # Step 4: Convert resampled data to a DataFrame\n",
    "# resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "# resampled_data[\"y\"] = y_resampled\n",
    "#\n",
    "# # Step 5: Save only the resampled data\n",
    "# resampled_data.to_csv(\"resampled_dataset_testing.csv\", index=False)\n",
    "#\n",
    "# # Step 6: Verify class balance\n",
    "# unique_counts = resampled_data[\"y\"].value_counts()\n",
    "# print(\"Class distribution in the resampled dataset:\")\n",
    "# print(unique_counts)\n",
    "#\n",
    "# # Step 7: Check if the counts are equal\n",
    "# if len(unique_counts) == 2 and unique_counts[0] == unique_counts[1]:\n",
    "#     print(\"Prediction column has equal count for both classes.\")\n",
    "# else:\n",
    "#     print(\"Class imbalance remains in the saved dataset.\")"
   ],
   "id": "30b0b09cfe9345ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in the resampled dataset:\n",
      "y\n",
      "0    30659\n",
      "1    30659\n",
      "Name: count, dtype: int64\n",
      "Prediction column has equal count for both classes.\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
