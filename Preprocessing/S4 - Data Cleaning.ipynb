{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LahiWeer/2330902-ML_CW/blob/master/Preprocessing/S4%20-%20Data%20Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "k-i76H2gp_7U",
        "ExecuteTime": {
          "end_time": "2024-12-25T09:32:09.747538Z",
          "start_time": "2024-12-25T09:32:07.354755Z"
        }
      },
      "id": "k-i76H2gp_7U",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "initial_id",
        "outputId": "9302a745-4203-49e7-a3d4-a21991652998",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "end_time": "2024-12-25T09:03:13.306407Z",
          "start_time": "2024-12-25T09:03:10.342005Z"
        }
      },
      "cell_type": "code",
      "source": [
        "data_url = 'https://raw.githubusercontent.com/LahiWeer/2330902-ML_CW/refs/heads/master/Preprocessing/Processed%20data%20in%20each%20step/processed_bank_data_final.csv'\n",
        "df = pd.read_csv(data_url)\n",
        "\n",
        "print(df['y'].value_counts())\n",
        "print(f\"\\nTotal rows in the original DataFrame: {len(df)}\")"
      ],
      "id": "initial_id",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y\n",
            "0    36193\n",
            "1     4594\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Total rows in the original DataFrame: 40787\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "6a15e6f85f863125",
        "jupyter": {
          "is_executing": true
        },
        "ExecuteTime": {
          "start_time": "2024-12-25T09:07:23.710384Z"
        }
      },
      "cell_type": "code",
      "source": [
        "# Find duplicate rows and mark the first occurrence index for duplicates\n",
        "duplicates = df[df.duplicated(keep=False)]  # Get all duplicated rows\n",
        "\n",
        "# Add a new column 'is_duplicate' and 'duplicate_with' to highlight duplicates and show their first occurrence\n",
        "df['is_duplicate'] = ''\n",
        "df['duplicate_with'] = ''\n",
        "\n",
        "# Get the index of the first occurrence of each duplicate\n",
        "first_occurrence_index = df[df.duplicated(keep='first')].index\n",
        "\n",
        "# Loop through the DataFrame to mark duplicates and their first occurrence\n",
        "for idx in first_occurrence_index:\n",
        "    # Find the first occurrence index\n",
        "    first_occurrence = df[df.iloc[:, :].eq(df.iloc[idx, :]).all(axis=1)].index[0]\n",
        "\n",
        "    # Mark the duplicate row and the first occurrence\n",
        "    df.at[idx, 'is_duplicate'] = 'Duplicate'\n",
        "    df.at[idx, 'duplicate_with'] = f\"Row {first_occurrence}\""
      ],
      "id": "6a15e6f85f863125",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-22T14:32:40.076726Z",
          "start_time": "2024-12-22T14:32:40.065098Z"
        },
        "id": "b0aab2476e924ad6",
        "outputId": "02b06ad8-1d73-4d10-e1f1-ba0d0d932f30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame with 'is_duplicate' and 'duplicate_with' columns:\n",
            "   age  job_blue-collar  job_entrepreneur  job_housemaid  job_management  \\\n",
            "0   56                0                 0              1               0   \n",
            "1   57                0                 0              0               0   \n",
            "2   37                0                 0              0               0   \n",
            "3   40                0                 0              0               0   \n",
            "4   56                0                 0              0               0   \n",
            "\n",
            "   job_retired  job_self-employed  job_services  job_student  job_technician  \\\n",
            "0            0                  0             0            0               0   \n",
            "1            0                  0             1            0               0   \n",
            "2            0                  0             1            0               0   \n",
            "3            0                  0             0            0               0   \n",
            "4            0                  0             1            0               0   \n",
            "\n",
            "   ...  previous  poutcome  emp.var.rate  cons.price.idx  cons.conf.idx  \\\n",
            "0  ...         0        -1           1.1          93.994          -36.4   \n",
            "1  ...         0        -1           1.1          93.994          -36.4   \n",
            "2  ...         0        -1           1.1          93.994          -36.4   \n",
            "3  ...         0        -1           1.1          93.994          -36.4   \n",
            "4  ...         0        -1           1.1          93.994          -36.4   \n",
            "\n",
            "   euribor3m  nr.employed  y  is_duplicate  duplicate_with  \n",
            "0      4.857       5191.0  0                                \n",
            "1      4.857       5191.0  0                                \n",
            "2      4.857       5191.0  0                                \n",
            "3      4.857       5191.0  0                                \n",
            "4      4.857       5191.0  0                                \n",
            "\n",
            "[5 rows x 33 columns]\n"
          ]
        }
      ],
      "execution_count": null,
      "source": [
        "# Display the DataFrame with the new 'is_duplicate' and 'duplicate_with' columns\n",
        "print(\"\\nDataFrame with 'is_duplicate' and 'duplicate_with' columns:\")\n",
        "print(df.head())  # Display the first few rows of the updated DataFrame"
      ],
      "id": "b0aab2476e924ad6"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-22T14:32:40.134848Z",
          "start_time": "2024-12-22T14:32:40.104309Z"
        },
        "id": "e2792885ed29458d",
        "outputId": "b9860d9e-fded-4a25-abaf-99daf61e744d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of duplicate rows: 277\n"
          ]
        }
      ],
      "execution_count": null,
      "source": [
        "# Count duplicates\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"\\nNumber of duplicate rows: {duplicate_count}\")"
      ],
      "id": "e2792885ed29458d"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-22T14:32:40.428523Z",
          "start_time": "2024-12-22T14:32:40.149651Z"
        },
        "id": "2b820b39ee611eb0",
        "outputId": "df9598c8-39ff-4ebd-9069-4dc4b1b0b440",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Updated DataFrame saved to: highlighted_duplicates_with_row_numbers.csv\n"
          ]
        }
      ],
      "execution_count": null,
      "source": [
        "# Save the updated DataFrame with the new columns to a CSV file\n",
        "output_file_path = 'highlighted_duplicates_with_row_numbers.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "print(f\"\\nUpdated DataFrame saved to: {output_file_path}\")"
      ],
      "id": "2b820b39ee611eb0"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-22T14:32:40.686378Z",
          "start_time": "2024-12-22T14:32:40.442820Z"
        },
        "id": "77ad6b1449053747",
        "outputId": "426328d6-9514-47b2-d296-b56ab9bc04dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total number of rows after removing duplicates: 38931\n",
            "\n",
            "Filtered DataFrame (without duplicates) saved to: cleaned_data_without_duplicates.csv\n"
          ]
        }
      ],
      "execution_count": null,
      "source": [
        "# Remove the rows that are marked as duplicates\n",
        "df_filtered = df[df['is_duplicate'] != 'Duplicate']\n",
        "\n",
        "# Count the total number of rows after removing duplicates\n",
        "total_rows_after_removal = df_filtered.shape[0]\n",
        "print(f\"\\nTotal number of rows after removing duplicates: {total_rows_after_removal}\")\n",
        "\n",
        "# Save the filtered DataFrame (without duplicates) to the original file or a new file\n",
        "filtered_file_path = 'cleaned_data_without_duplicates.csv'\n",
        "df_filtered.to_csv(filtered_file_path, index=False)\n",
        "print(f\"\\nFiltered DataFrame (without duplicates) saved to: {filtered_file_path}\")"
      ],
      "id": "77ad6b1449053747"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the uploaded CSV file to inspect its structure\n",
        "bank_data_url= 'https://raw.githubusercontent.com/LahiWeer/2330902-ML_CW/refs/heads/master/Preprocessing/Processed%20data%20in%20each%20step/cleaned_data_without_duplicates.csv'\n",
        "bank_data = pd.read_csv(bank_data_url)"
      ],
      "metadata": {
        "id": "Pc3l60oktm2l"
      },
      "id": "Pc3l60oktm2l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the initial row count\n",
        "initial_row_count = len(bank_data)\n",
        "print(f\"Initial row count: {initial_row_count}\")"
      ],
      "metadata": {
        "id": "tvXVjm3pwWxJ",
        "outputId": "9a34aa9f-9fe9-44df-fc0f-4e2d8a8a2635",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tvXVjm3pwWxJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial row count: 38931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "# Filter invalid rows where pdays = 999 and poutcome != -1\n",
        "invalid_rows = bank_data[(bank_data['pdays'] == 999) & (bank_data['poutcome'] != -1)]\n",
        "invalid_count = len(invalid_rows)\n",
        "total_rows = len(bank_data)\n",
        "removed_percentage = (invalid_count / total_rows) * 100"
      ],
      "metadata": {
        "id": "U2tnh2zowbHt"
      },
      "id": "U2tnh2zowbHt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the invalid rows to a separate CSV file\n",
        "invalid_rows.to_csv('invalid_rows_pdays_999.csv', index=False)\n",
        "print(f\"Saved {invalid_count} invalid rows to 'invalid_rows_pdays_999.csv'.\")"
      ],
      "metadata": {
        "id": "GbADtQzzwj0d",
        "outputId": "dde9b455-fbe4-4f6e-fa5d-ac14f70b7ab3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GbADtQzzwj0d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 4001 invalid rows to 'invalid_rows_pdays_999.csv'.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-23T12:02:06.144494Z",
          "start_time": "2024-12-23T12:01:58.237435Z"
        },
        "id": "381961d4906e753",
        "outputId": "08fbbd75-27c0-464d-b310-249a2d23e805",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row count of cleaned dataset: 34930\n",
            "Filtered 4001 invalid rows, which is 10.28% of the total dataset.\n"
          ]
        }
      ],
      "execution_count": null,
      "source": [
        "# Create a cleaned version of the dataset without altering the original dataset\n",
        "bank_data_cleaned = bank_data[~bank_data.index.isin(invalid_rows.index)]\n",
        "\n",
        "# Print the updated row count for the cleaned dataset\n",
        "final_row_count = len(bank_data_cleaned)\n",
        "print(f\"Row count of cleaned dataset: {final_row_count}\")\n",
        "\n",
        "# Summarize the percentage of invalid rows filtered\n",
        "print(f\"Filtered {invalid_count} invalid rows, which is {removed_percentage:.2f}% of the total dataset.\")"
      ],
      "id": "381961d4906e753"
    },
    {
      "metadata": {
        "id": "931d9c6e4b4e9629",
        "outputId": "f1d1f643-53a9-4018-b8d4-0da8839b915f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved the cleaned dataset to 'bank_data_cleaned_final.csv'.\n"
          ]
        }
      ],
      "execution_count": null,
      "source": [
        "# Save the cleaned dataset to a separate CSV file\n",
        "bank_data_cleaned.to_csv('bank_data_cleaned_final.csv', index=False)\n",
        "print(\"Saved the cleaned dataset to 'bank_data_cleaned_final.csv'.\")"
      ],
      "id": "931d9c6e4b4e9629"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df_url = 'https://raw.githubusercontent.com/LahiWeer/2330902-ML_CW/refs/heads/master/Preprocessing/Processed%20data%20in%20each%20step/bank_data_cleaned_final.csv'\n",
        "df= pd.read_csv(df_url)"
      ],
      "metadata": {
        "id": "0RLjZ6YYyCqW"
      },
      "id": "0RLjZ6YYyCqW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Keep a copy of the original dataset\n",
        "original_df = df.copy()\n",
        "\n",
        "# 2. Detect outliers in the 'campaign' column (campaign >= 17)\n",
        "outliers = df[df['campaign'] > 17]\n",
        "\n",
        "# 3. Remove outliers and create a new dataset\n",
        "df_no_outliers = df[df['campaign'] <= 17]\n",
        "df_no_outliers.to_csv('Updated_cleaned_data_no_outliers.csv', index=False)\n",
        "\n",
        "# Notify the user\n",
        "print(\"Outliers have been removed and the cleaned dataset is saved as 'Updated_cleaned_data_no_outliers.csv'.\")"
      ],
      "metadata": {
        "id": "gDnFttBGzVhv",
        "outputId": "50b2c412-7673-4f7d-a2f1-bbacddd0cabe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "gDnFttBGzVhv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outliers have been removed and the cleaned dataset is saved as 'Updated_cleaned_data_no_outliers.csv'.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "a9d2a4393e663764",
        "outputId": "bbe7ef88-4578-4934-9c59-888a25053737",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "ExecuteTime": {
          "end_time": "2024-12-25T09:32:24.325182Z",
          "start_time": "2024-12-25T09:32:17.621765Z"
        }
      },
      "cell_type": "code",
      "source": [
        "def balance_dataset_with_smote(original_file, target_column, output_file):\n",
        "    # Step 1: Load the Original Dataset\n",
        "    data = pd.read_csv(original_file)\n",
        "    X = data.drop(columns=[target_column])\n",
        "    y = data[target_column]\n",
        "\n",
        "    # Step 2: Analyze Class Distribution\n",
        "    class_counts = y.value_counts()\n",
        "    count_y0, count_y1 = class_counts[0], class_counts[1]\n",
        "    rows_to_add_y1 = count_y0 - count_y1\n",
        "\n",
        "    # Step 3: Check if Resampling is Required\n",
        "    if rows_to_add_y1 <= 0:\n",
        "        print(\"Dataset is already balanced.\")\n",
        "        data.to_csv(output_file, index=False)\n",
        "        return\n",
        "\n",
        "    # Step 4: Reapply SMOTE to Generate Synthetic Data\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "    resampled_data[target_column] = y_resampled\n",
        "\n",
        "    # Save the original data\n",
        "    original_data = data.copy()\n",
        "    original_data.to_csv(output_file, index=False)\n",
        "\n",
        "    # Step 5: Filter and Prepare the Minority Class Rows\n",
        "    resampled_data_y1 = resampled_data[resampled_data[target_column] == 1]\n",
        "\n",
        "    # Step 6: Check for Duplicates Before Appending\n",
        "    resampled_and_original_data = pd.read_csv(output_file)\n",
        "\n",
        "    # Remove duplicates using concat and drop_duplicates\n",
        "    combined_data = pd.concat([resampled_and_original_data, resampled_data_y1])\n",
        "    combined_data = combined_data.drop_duplicates(keep=False)\n",
        "\n",
        "    # Append only the required number of non-duplicate rows\n",
        "    resampled_data_y1_to_append = combined_data[combined_data[target_column] == 1].head(rows_to_add_y1)\n",
        "    resampled_and_original_data = pd.concat(\n",
        "        [resampled_and_original_data, resampled_data_y1_to_append], ignore_index=True\n",
        "    )\n",
        "    resampled_and_original_data.to_csv(output_file, index=False)\n",
        "\n",
        "    # Step 7: Check Final Balance and Reapply SMOTE if Necessary\n",
        "    while True:\n",
        "        final_class_counts = resampled_and_original_data[target_column].value_counts()\n",
        "        count_y0, count_y1 = final_class_counts[0], final_class_counts[1]\n",
        "\n",
        "        if count_y0 == count_y1:\n",
        "            break  # Dataset is balanced; no further action required.\n",
        "\n",
        "        # Reapply SMOTE on the combined dataset\n",
        "        X = resampled_and_original_data.drop(columns=[target_column])\n",
        "        y = resampled_and_original_data[target_column]\n",
        "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "        resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "        resampled_data[target_column] = y_resampled\n",
        "\n",
        "        # Filter the newly generated y == 1 rows\n",
        "        resampled_data_y1 = resampled_data[resampled_data[target_column] == 1]\n",
        "\n",
        "        # Remove duplicates using concat and drop_duplicates\n",
        "        combined_data = pd.concat([resampled_and_original_data, resampled_data_y1])\n",
        "        combined_data = combined_data.drop_duplicates(keep=False)\n",
        "\n",
        "        # Append only the required number of non-duplicate rows\n",
        "        rows_to_add_y1 = count_y0 - count_y1\n",
        "        resampled_data_y1_to_append = combined_data[combined_data[target_column] == 1].head(rows_to_add_y1)\n",
        "        resampled_and_original_data = pd.concat(\n",
        "            [resampled_and_original_data, resampled_data_y1_to_append], ignore_index=True\n",
        "        )\n",
        "        resampled_and_original_data.to_csv(output_file, index=False)\n",
        "\n",
        "    # Step 8: Verify Final Class Distribution\n",
        "    final_class_counts = resampled_and_original_data[target_column].value_counts()\n",
        "    print(f\"Final Class Distribution:\\n{final_class_counts}\")\n",
        "    print(f\"Balanced dataset saved to '{output_file}'.\")\n",
        "\n",
        "# Input file paths and target column\n",
        "original_file = \"https://raw.githubusercontent.com/LahiWeer/2330902-ML_CW/refs/heads/master/Preprocessing/Processed%20data%20in%20each%20step/Updated_cleaned_data_no_outliers.csv\"\n",
        "output_file = \"resampled_and_original_data.csv\"\n",
        "target_column = \"y\"\n",
        "\n",
        "# Balance the dataset\n",
        "balance_dataset_with_smote(original_file, target_column, output_file)"
      ],
      "id": "a9d2a4393e663764",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Class Distribution:\n",
            "y\n",
            "0    30659\n",
            "1    30659\n",
            "Name: count, dtype: int64\n",
            "Balanced dataset saved to 'resampled_and_original_data.csv'.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "d07afb572aa10f60",
        "outputId": "f3bbab6c-22f7-4625-8334-6c708479b08b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "ExecuteTime": {
          "end_time": "2024-12-25T09:32:30.530700Z",
          "start_time": "2024-12-25T09:32:26.742316Z"
        }
      },
      "cell_type": "code",
      "source": [
        "check_equal_data = pd.read_csv(\"https://raw.githubusercontent.com/LahiWeer/2330902-ML_CW/refs/heads/master/Preprocessing/resampled_and_original_data.csv\")\n",
        "check_equal_data['y'].value_counts()"
      ],
      "id": "d07afb572aa10f60",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "y\n",
              "0    30659\n",
              "1    30659\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ca170ead3ca0fe7e",
        "outputId": "072f6be7-bdb6-4732-ccc4-c9a0cc48d8f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "end_time": "2024-12-25T09:32:30.660254Z",
          "start_time": "2024-12-25T09:32:30.576957Z"
        }
      },
      "cell_type": "code",
      "source": [
        "# Find duplicate rows\n",
        "duplicates = check_equal_data[check_equal_data.duplicated(keep=False)]  # Get all duplicated rows\n",
        "\n",
        "# Count duplicates\n",
        "duplicate_count = check_equal_data.duplicated().sum()\n",
        "print(f\"\\nNumber of duplicate rows: {duplicate_count}\")"
      ],
      "id": "ca170ead3ca0fe7e",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Number of duplicate rows: 0\n"
          ]
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}