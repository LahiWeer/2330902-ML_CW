{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-22T16:56:32.443693Z",
     "start_time": "2024-12-22T16:56:31.911304Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "0    36193\n",
       "1     4594\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'processed_bank_data_final.csv'  # Replace with your CSV file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display initial DataFrame information\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.head())  # Display the first few rows\n",
    "print(f\"\\nTotal rows in the original DataFrame: {len(df)}\")"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T14:32:39.942899Z",
     "start_time": "2024-12-22T14:31:18.604307Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "# Find duplicate rows and mark the first occurrence index for duplicates\n",
    "duplicates = df[df.duplicated(keep=False)]  # Get all duplicated rows\n",
    "\n",
    "# Add a new column 'is_duplicate' and 'duplicate_with' to highlight duplicates and show their first occurrence\n",
    "df['is_duplicate'] = ''\n",
    "df['duplicate_with'] = ''\n",
    "\n",
    "# Get the index of the first occurrence of each duplicate\n",
    "first_occurrence_index = df[df.duplicated(keep='first')].index\n",
    "\n",
    "# Loop through the DataFrame to mark duplicates and their first occurrence\n",
    "for idx in first_occurrence_index:\n",
    "    # Find the first occurrence index\n",
    "    first_occurrence = df[df.iloc[:, :].eq(df.iloc[idx, :]).all(axis=1)].index[0]\n",
    "\n",
    "    # Mark the duplicate row and the first occurrence\n",
    "    df.at[idx, 'is_duplicate'] = 'Duplicate'\n",
    "    df.at[idx, 'duplicate_with'] = f\"Row {first_occurrence}\""
   ],
   "id": "6a15e6f85f863125"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T14:32:40.076726Z",
     "start_time": "2024-12-22T14:32:40.065098Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with 'is_duplicate' and 'duplicate_with' columns:\n",
      "   age  job_blue-collar  job_entrepreneur  job_housemaid  job_management  \\\n",
      "0   56                0                 0              1               0   \n",
      "1   57                0                 0              0               0   \n",
      "2   37                0                 0              0               0   \n",
      "3   40                0                 0              0               0   \n",
      "4   56                0                 0              0               0   \n",
      "\n",
      "   job_retired  job_self-employed  job_services  job_student  job_technician  \\\n",
      "0            0                  0             0            0               0   \n",
      "1            0                  0             1            0               0   \n",
      "2            0                  0             1            0               0   \n",
      "3            0                  0             0            0               0   \n",
      "4            0                  0             1            0               0   \n",
      "\n",
      "   ...  previous  poutcome  emp.var.rate  cons.price.idx  cons.conf.idx  \\\n",
      "0  ...         0        -1           1.1          93.994          -36.4   \n",
      "1  ...         0        -1           1.1          93.994          -36.4   \n",
      "2  ...         0        -1           1.1          93.994          -36.4   \n",
      "3  ...         0        -1           1.1          93.994          -36.4   \n",
      "4  ...         0        -1           1.1          93.994          -36.4   \n",
      "\n",
      "   euribor3m  nr.employed  y  is_duplicate  duplicate_with  \n",
      "0      4.857       5191.0  0                                \n",
      "1      4.857       5191.0  0                                \n",
      "2      4.857       5191.0  0                                \n",
      "3      4.857       5191.0  0                                \n",
      "4      4.857       5191.0  0                                \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "execution_count": 3,
   "source": [
    "# Display the DataFrame with the new 'is_duplicate' and 'duplicate_with' columns\n",
    "print(\"\\nDataFrame with 'is_duplicate' and 'duplicate_with' columns:\")\n",
    "print(df.head())  # Display the first few rows of the updated DataFrame"
   ],
   "id": "b0aab2476e924ad6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T14:32:40.134848Z",
     "start_time": "2024-12-22T14:32:40.104309Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows: 277\n"
     ]
    }
   ],
   "execution_count": 4,
   "source": [
    "# Count duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicate_count}\")"
   ],
   "id": "e2792885ed29458d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T14:32:40.428523Z",
     "start_time": "2024-12-22T14:32:40.149651Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated DataFrame saved to: highlighted_duplicates_with_row_numbers.csv\n"
     ]
    }
   ],
   "execution_count": 5,
   "source": [
    "# Save the updated DataFrame with the new columns to a CSV file\n",
    "output_file_path = 'For Reference/highlighted_duplicates_with_row_numbers.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "print(f\"\\nUpdated DataFrame saved to: {output_file_path}\")"
   ],
   "id": "2b820b39ee611eb0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T14:32:40.686378Z",
     "start_time": "2024-12-22T14:32:40.442820Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of rows after removing duplicates: 38931\n",
      "\n",
      "Filtered DataFrame (without duplicates) saved to: cleaned data.csv\n"
     ]
    }
   ],
   "execution_count": 6,
   "source": [
    "# Remove the rows that are marked as duplicates\n",
    "df_filtered = df[df['is_duplicate'] != 'Duplicate']\n",
    "\n",
    "# Count the total number of rows after removing duplicates\n",
    "total_rows_after_removal = df_filtered.shape[0]\n",
    "print(f\"\\nTotal number of rows after removing duplicates: {total_rows_after_removal}\")\n",
    "\n",
    "# Save the filtered DataFrame (without duplicates) to the original file or a new file\n",
    "filtered_file_path = 'cleaned_data_without_duplicates.csv'  # Replace with the desired output file path\n",
    "df_filtered.to_csv(filtered_file_path, index=False)\n",
    "print(f\"\\nFiltered DataFrame (without duplicates) saved to: {filtered_file_path}\")"
   ],
   "id": "77ad6b1449053747"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:02:06.144494Z",
     "start_time": "2024-12-23T12:01:58.237435Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial row count: 40787\n",
      "Saved 4080 invalid rows to 'invalid_rows_pdays_999.csv'.\n",
      "Row count of cleaned dataset: 36707\n",
      "Random Forest - Original Data:\n",
      "Accuracy: 0.89\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94     10853\n",
      "           1       0.56      0.28      0.38      1384\n",
      "\n",
      "    accuracy                           0.89     12237\n",
      "   macro avg       0.74      0.63      0.66     12237\n",
      "weighted avg       0.87      0.89      0.88     12237\n",
      "\n",
      "Neural Network - Original Data:\n",
      "Accuracy: 0.88\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     10853\n",
      "           1       0.47      0.41      0.44      1384\n",
      "\n",
      "    accuracy                           0.88     12237\n",
      "   macro avg       0.70      0.68      0.69     12237\n",
      "weighted avg       0.87      0.88      0.88     12237\n",
      "\n",
      "Random Forest - Cleaned Data:\n",
      "Accuracy: 0.90\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94      9810\n",
      "           1       0.56      0.31      0.40      1203\n",
      "\n",
      "    accuracy                           0.90     11013\n",
      "   macro avg       0.74      0.64      0.67     11013\n",
      "weighted avg       0.88      0.90      0.89     11013\n",
      "\n",
      "Neural Network - Cleaned Data:\n",
      "Accuracy: 0.90\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95      9810\n",
      "           1       0.60      0.26      0.37      1203\n",
      "\n",
      "    accuracy                           0.90     11013\n",
      "   macro avg       0.76      0.62      0.66     11013\n",
      "weighted avg       0.88      0.90      0.88     11013\n",
      "\n",
      "Filtered 4080 invalid rows, which is 10.00% of the total dataset.\n"
     ]
    }
   ],
   "execution_count": 43,
   "source": [
    "# Import necessary libraries for Random Forest and Neural Network models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file to inspect its structure\n",
    "file_path = 'cleaned_data_without_duplicates.csv'\n",
    "bank_data = pd.read_csv(file_path)\n",
    "\n",
    "# Print the initial row count\n",
    "initial_row_count = len(bank_data)\n",
    "print(f\"Initial row count: {initial_row_count}\")\n",
    "\n",
    "# Preprocess the data\n",
    "# Filter invalid rows where pdays = 999 and poutcome != -1\n",
    "invalid_rows = bank_data[(bank_data['pdays'] == 999) & (bank_data['poutcome'] != -1)]\n",
    "invalid_count = len(invalid_rows)\n",
    "total_rows = len(bank_data)\n",
    "removed_percentage = (invalid_count / total_rows) * 100\n",
    "\n",
    "# Save the invalid rows to a separate CSV file\n",
    "invalid_rows.to_csv('invalid_rows_pdays_999.csv', index=False)\n",
    "print(f\"Saved {invalid_count} invalid rows to 'invalid_rows_pdays_999.csv'.\")\n",
    "\n",
    "# Create a cleaned version of the dataset without altering the original dataset\n",
    "bank_data_cleaned = bank_data[~bank_data.index.isin(invalid_rows.index)]\n",
    "\n",
    "# Print the updated row count for the cleaned dataset\n",
    "final_row_count = len(bank_data_cleaned)\n",
    "print(f\"Row count of cleaned dataset: {final_row_count}\")\n",
    "\n",
    "# Train-test split (same for both datasets)\n",
    "X = bank_data.drop('y', axis=1)  # Assuming 'target' is the target variable\n",
    "y = bank_data['y']\n",
    "\n",
    "X_cleaned = bank_data_cleaned.drop('y', axis=1)\n",
    "y_cleaned = bank_data_cleaned['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.3, random_state=42)\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return accuracy, report\n",
    "\n",
    "# 1. Random Forest Model (on original data)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "accuracy_rf, report_rf = evaluate_model(rf_model, X_train, X_test, y_train, y_test)\n",
    "print(\"Random Forest - Original Data:\")\n",
    "print(f\"Accuracy: {accuracy_rf:.2f}\")\n",
    "print(report_rf)\n",
    "\n",
    "# 2. Neural Network Model (on original data)\n",
    "nn_model = MLPClassifier(random_state=42, max_iter=500)\n",
    "accuracy_nn, report_nn = evaluate_model(nn_model, X_train, X_test, y_train, y_test)\n",
    "print(\"Neural Network - Original Data:\")\n",
    "print(f\"Accuracy: {accuracy_nn:.2f}\")\n",
    "print(report_nn)\n",
    "\n",
    "# 3. Random Forest Model (on cleaned data)\n",
    "accuracy_rf_cleaned, report_rf_cleaned = evaluate_model(rf_model, X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned)\n",
    "print(\"Random Forest - Cleaned Data:\")\n",
    "print(f\"Accuracy: {accuracy_rf_cleaned:.2f}\")\n",
    "print(report_rf_cleaned)\n",
    "\n",
    "# 4. Neural Network Model (on cleaned data)\n",
    "accuracy_nn_cleaned, report_nn_cleaned = evaluate_model(nn_model, X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned)\n",
    "print(\"Neural Network - Cleaned Data:\")\n",
    "print(f\"Accuracy: {accuracy_nn_cleaned:.2f}\")\n",
    "print(report_nn_cleaned)\n",
    "\n",
    "# Summarize the percentage of invalid rows filtered\n",
    "print(f\"Filtered {invalid_count} invalid rows, which is {removed_percentage:.2f}% of the total dataset.\")"
   ],
   "id": "381961d4906e753"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the cleaned dataset to a separate CSV file\n",
    "bank_data_cleaned.to_csv('bank_data_cleaned_final.csv', index=False)\n",
    "print(\"Saved the cleaned dataset to 'bank_data_cleaned_final.csv'.\")"
   ],
   "id": "931d9c6e4b4e9629"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T03:49:33.785493Z",
     "start_time": "2024-12-23T03:49:25.580134Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the original dataset:\n",
      "{'Random Forest': 0.8910678499856857, 'Neural Network': 0.8866304036644718}\n",
      "Accuracy on the dataset without outliers:\n",
      "{'Random Forest': 0.8943499567598732, 'Neural Network': 0.8731622946093975}\n"
     ]
    }
   ],
   "execution_count": 39,
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"bank_data_cleaned_final.csv\")\n",
    "\n",
    "# Define a function to train and evaluate models\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    results = {}\n",
    "\n",
    "    # Random Forest Model\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_rf = rf.predict(X_test)\n",
    "    results['Random Forest'] = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "    # Neural Network Model\n",
    "    nn = MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=500, random_state=42)\n",
    "    nn.fit(X_train, y_train)\n",
    "    y_pred_nn = nn.predict(X_test)\n",
    "    results['Neural Network'] = accuracy_score(y_test, y_pred_nn)\n",
    "\n",
    "    return results\n",
    "\n",
    "# 1. Keep a copy of the original dataset\n",
    "original_df = df.copy()\n",
    "\n",
    "# 2. Detect outliers in the 'campaign' column (campaign >= 17)\n",
    "outliers = df[df['campaign'] > 17]\n",
    "\n",
    "# 3. Remove outliers and create a new dataset\n",
    "df_no_outliers = df[df['campaign'] <= 17]\n",
    "df_no_outliers.to_csv('Updated_cleaned_data_no_outliers.csv', index=False)\n",
    "\n",
    "# 4. Evaluate models on the original dataset\n",
    "# Prepare the data\n",
    "X = original_df.drop(columns=['y'])\n",
    "y = original_df['y']\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate models\n",
    "original_results = train_and_evaluate(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy on the original dataset:\")\n",
    "print(original_results)\n",
    "\n",
    "# 5. Evaluate models on the dataset without outliers\n",
    "# Prepare the data\n",
    "X_no_outliers = df_no_outliers.drop(columns=['y'])\n",
    "y_no_outliers = df_no_outliers['y']\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X_no_outliers = pd.get_dummies(X_no_outliers, drop_first=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_no_outliers, X_test_no_outliers, y_train_no_outliers, y_test_no_outliers = train_test_split(\n",
    "    X_no_outliers, y_no_outliers, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train and evaluate models\n",
    "updated_results = train_and_evaluate(X_train_no_outliers, X_test_no_outliers, y_train_no_outliers, y_test_no_outliers)\n",
    "print(\"Accuracy on the dataset without outliers:\")\n",
    "print(updated_results)"
   ],
   "id": "c350f85d36db2682"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def balance_dataset_with_smote(original_file, target_column, output_file):\n",
    "    # Step 1: Load the Original Dataset\n",
    "    data = pd.read_csv(original_file)\n",
    "    X = data.drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "\n",
    "    # Step 2: Analyze Class Distribution\n",
    "    class_counts = y.value_counts()\n",
    "    count_y0, count_y1 = class_counts[0], class_counts[1]\n",
    "    rows_to_add_y1 = count_y0 - count_y1\n",
    "\n",
    "    # Step 3: Check if Resampling is Required\n",
    "    if rows_to_add_y1 <= 0:\n",
    "        print(\"Dataset is already balanced.\")\n",
    "        data.to_csv(output_file, index=False)\n",
    "        return\n",
    "\n",
    "    # Step 4: Reapply SMOTE to Generate Synthetic Data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    resampled_data[target_column] = y_resampled\n",
    "\n",
    "    # Save the original data\n",
    "    original_data = data.copy()\n",
    "    original_data.to_csv(output_file, index=False)\n",
    "\n",
    "    # Step 5: Filter and Prepare the Minority Class Rows\n",
    "    resampled_data_y1 = resampled_data[resampled_data[target_column] == 1]\n",
    "\n",
    "    # Step 6: Check for Duplicates Before Appending\n",
    "    resampled_and_original_data = pd.read_csv(output_file)\n",
    "\n",
    "    # Remove duplicates using concat and drop_duplicates\n",
    "    combined_data = pd.concat([resampled_and_original_data, resampled_data_y1])\n",
    "    combined_data = combined_data.drop_duplicates(keep=False)\n",
    "\n",
    "    # Append only the required number of non-duplicate rows\n",
    "    resampled_data_y1_to_append = combined_data[combined_data[target_column] == 1].head(rows_to_add_y1)\n",
    "    resampled_and_original_data = pd.concat(\n",
    "        [resampled_and_original_data, resampled_data_y1_to_append], ignore_index=True\n",
    "    )\n",
    "    resampled_and_original_data.to_csv(output_file, index=False)\n",
    "\n",
    "    # Step 7: Check Final Balance and Reapply SMOTE if Necessary\n",
    "    while True:\n",
    "        final_class_counts = resampled_and_original_data[target_column].value_counts()\n",
    "        count_y0, count_y1 = final_class_counts[0], final_class_counts[1]\n",
    "\n",
    "        if count_y0 == count_y1:\n",
    "            break  # Dataset is balanced; no further action required.\n",
    "\n",
    "        # Reapply SMOTE on the combined dataset\n",
    "        X = resampled_and_original_data.drop(columns=[target_column])\n",
    "        y = resampled_and_original_data[target_column]\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "        resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "        resampled_data[target_column] = y_resampled\n",
    "\n",
    "        # Filter the newly generated y == 1 rows\n",
    "        resampled_data_y1 = resampled_data[resampled_data[target_column] == 1]\n",
    "\n",
    "        # Remove duplicates using concat and drop_duplicates\n",
    "        combined_data = pd.concat([resampled_and_original_data, resampled_data_y1])\n",
    "        combined_data = combined_data.drop_duplicates(keep=False)\n",
    "\n",
    "        # Append only the required number of non-duplicate rows\n",
    "        rows_to_add_y1 = count_y0 - count_y1\n",
    "        resampled_data_y1_to_append = combined_data[combined_data[target_column] == 1].head(rows_to_add_y1)\n",
    "        resampled_and_original_data = pd.concat(\n",
    "            [resampled_and_original_data, resampled_data_y1_to_append], ignore_index=True\n",
    "        )\n",
    "        resampled_and_original_data.to_csv(output_file, index=False)\n",
    "\n",
    "    # Step 8: Verify Final Class Distribution\n",
    "    final_class_counts = resampled_and_original_data[target_column].value_counts()\n",
    "    print(f\"Final Class Distribution:\\n{final_class_counts}\")\n",
    "    print(f\"Balanced dataset saved to '{output_file}'.\")\n",
    "\n",
    "# Input file paths and target column\n",
    "original_file = \"Updated_cleaned_data_no_outliers.csv\"\n",
    "output_file = \"resampled_and_original_data.csv\"\n",
    "target_column = \"y\"\n",
    "\n",
    "# Balance the dataset\n",
    "balance_dataset_with_smote(original_file, target_column, output_file)"
   ],
   "id": "a9d2a4393e663764",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T01:42:16.950589Z",
     "start_time": "2024-12-24T01:42:16.481835Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "0    24527\n",
       "1    24527\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 113,
   "source": [
    "data = pd.read_csv(\"resampled_and_original_data.csv\")\n",
    "data['y'].value_counts()"
   ],
   "id": "d07afb572aa10f60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T07:38:35.318598Z",
     "start_time": "2024-12-24T07:38:35.157455Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "execution_count": 16,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'resampled_and_original_data.csv'  # Replace with your CSV file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Find duplicate rows\n",
    "duplicates = df[df.duplicated(keep=False)]  # Get all duplicated rows\n",
    "\n",
    "# Count duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicate_count}\")"
   ],
   "id": "ca170ead3ca0fe7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T17:36:50.940318Z",
     "start_time": "2024-12-23T17:36:49.475330Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled and original data saved to 'resampled_and_original_data.csv'\n"
     ]
    }
   ],
   "execution_count": 100,
   "source": [
    "# import pandas as pd\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# # Example: Load your dataset\n",
    "# data = pd.read_csv(\"Updated_cleaned_data_no_outliers.csv\")\n",
    "#\n",
    "# # Split features and target\n",
    "# X = data.drop(columns=[\"y\"])  # Features\n",
    "# y = data[\"y\"]                 # Target\n",
    "#\n",
    "# # Apply train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "#\n",
    "# # Visualize original class distribution\n",
    "# original_counts = y_train.value_counts()\n",
    "#\n",
    "# # Apply SMOTE\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "#\n",
    "# # Convert resampled data to DataFrame\n",
    "# X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "# y_resampled_df = pd.DataFrame(y_resampled, columns=[\"y\"])\n",
    "#\n",
    "# # Combine features and target for resampled data\n",
    "# resampled_data = pd.concat([X_resampled_df, y_resampled_df], axis=1)\n",
    "#\n",
    "# # Filter the rows where y == 1\n",
    "# resampled_data_y1 = resampled_data[resampled_data[\"y\"] == 1]\n",
    "#\n",
    "# # Calculate the number of y == 1 rows to append to make counts equal\n",
    "# count_y0 = original_counts[0]  # Count of y == 0 in original data\n",
    "# count_y1 = original_counts[1]  # Count of y == 1 in original data\n",
    "# rows_to_add_y1 = count_y0 - count_y1  # The difference, i.e., how many y == 1 rows to append\n",
    "#\n",
    "# # Take only the necessary number of y == 1 rows from resampled data\n",
    "# resampled_data_y1_to_append = resampled_data_y1.head(rows_to_add_y1)\n",
    "#\n",
    "# # Combine original data with only the necessary y == 1 rows from the resampled data\n",
    "# original_data = pd.concat([X_train, y_train], axis=1)\n",
    "# combined_data = pd.concat([original_data, resampled_data_y1_to_append], ignore_index=True)\n",
    "#\n",
    "# # Save to CSV\n",
    "# combined_data.to_csv(\"resampled_and_original_data.csv\", index=False)\n",
    "#\n",
    "# print(\"Resampled and original data saved to 'resampled_and_original_data.csv'\")"
   ],
   "id": "1717d1b13c8e87a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T04:15:17.600888Z",
     "start_time": "2024-12-24T04:15:12.347897Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Class distribution:\n",
      "y\n",
      "0    24527\n",
      "1     3222\n",
      "Name: count, dtype: int64\n",
      "Iteration 1: Class distribution:\n",
      "y\n",
      "0    24527\n",
      "1    22072\n",
      "Name: count, dtype: int64\n",
      "Iteration 2: Class distribution:\n",
      "y\n",
      "0    24527\n",
      "1    24372\n",
      "Name: count, dtype: int64\n",
      "Iteration 3: Class distribution:\n",
      "y\n",
      "0    24527\n",
      "1    24510\n",
      "Name: count, dtype: int64\n",
      "Iteration 4: Class distribution:\n",
      "y\n",
      "0    24527\n",
      "1    24522\n",
      "Name: count, dtype: int64\n",
      "Iteration 5: Class distribution:\n",
      "y\n",
      "0    24527\n",
      "1    24526\n",
      "Name: count, dtype: int64\n",
      "Iteration 6: Class distribution:\n",
      "y\n",
      "0    24527\n",
      "1    24527\n",
      "Name: count, dtype: int64\n",
      "Balanced dataset achieved.\n",
      "Final balanced dataset saved to 'resampled_and_original_data1.csv'\n",
      "Final class distribution after balancing:\n",
      " y\n",
      "0    24527\n",
      "1    24527\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "# import pandas as pd\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# # Function to resample data until balance is achieved\n",
    "# def ensure_balanced_data(data, target_column, output_file):\n",
    "#     smote = SMOTE(random_state=42)\n",
    "#     max_iterations = 10  # Limit iterations to avoid infinite loops in edge cases\n",
    "#     iteration = 0\n",
    "#\n",
    "#     while iteration < max_iterations:\n",
    "#         # Check current class distribution\n",
    "#         class_counts = data[target_column].value_counts()\n",
    "#         print(f\"Iteration {iteration}: Class distribution:\\n{class_counts}\")\n",
    "#\n",
    "#         if class_counts.min() == class_counts.max():\n",
    "#             print(\"Balanced dataset achieved.\")\n",
    "#             break\n",
    "#\n",
    "#         # Determine the majority and minority class\n",
    "#         majority_class = class_counts.idxmax()\n",
    "#         minority_class = class_counts.idxmin()\n",
    "#\n",
    "#         # Separate features and target\n",
    "#         X = data.drop(columns=[target_column])\n",
    "#         y = data[target_column]\n",
    "#\n",
    "#         # Resample to balance classes\n",
    "#         sampling_strategy = {minority_class: class_counts[majority_class]}\n",
    "#         X_resampled, y_resampled = SMOTE(random_state=42, sampling_strategy=sampling_strategy).fit_resample(X, y)\n",
    "#\n",
    "#         # Combine resampled features and target\n",
    "#         resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "#         resampled_data[target_column] = y_resampled\n",
    "#\n",
    "#         # Remove duplicates\n",
    "#         resampled_data = resampled_data.drop_duplicates(keep=\"first\")\n",
    "#\n",
    "#         # Update the dataset with resampled data\n",
    "#         data = pd.concat([data, resampled_data], ignore_index=True).drop_duplicates(keep=\"first\")\n",
    "#         iteration += 1\n",
    "#\n",
    "#     # Save the balanced dataset\n",
    "#     data.to_csv(output_file, index=False)\n",
    "#     print(f\"Final balanced dataset saved to '{output_file}'\")\n",
    "#\n",
    "#     # Verify final class distribution\n",
    "#     final_counts = data[target_column].value_counts()\n",
    "#     print(\"Final class distribution after balancing:\\n\", final_counts)\n",
    "#\n",
    "# # Load the original cleaned data\n",
    "# original_data = pd.read_csv(\"Updated_cleaned_data_no_outliers.csv\")\n",
    "#\n",
    "# # Split features and target\n",
    "# X = original_data.drop(columns=[\"y\"])  # Features\n",
    "# y = original_data[\"y\"]                 # Target\n",
    "#\n",
    "# # Apply train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "#\n",
    "# # Combine training data\n",
    "# train_data = pd.concat([X_train, y_train], axis=1)\n",
    "#\n",
    "# # Save initial combined dataset and ensure balance\n",
    "# output_file = \"resampled_and_original_data1.csv\"\n",
    "# ensure_balanced_data(train_data, target_column=\"y\", output_file=output_file)"
   ],
   "id": "2b7408bbfdc5689b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:26:00.569571Z",
     "start_time": "2024-12-23T18:25:59.763560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import pandas as pd\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "#\n",
    "# # Step 1: Load the dataset\n",
    "# data = pd.read_csv(\"Updated_cleaned_data_no_outliers.csv\")\n",
    "#\n",
    "# # Step 2: Separate features and target\n",
    "# X = data.drop(columns=[\"y\"])  # Features\n",
    "# y = data[\"y\"]                 # Target (prediction column)\n",
    "#\n",
    "# # Step 3: Apply SMOTE\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "#\n",
    "# # Step 4: Convert resampled data to a DataFrame\n",
    "# resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "# resampled_data[\"y\"] = y_resampled\n",
    "#\n",
    "# # Step 5: Save only the resampled data\n",
    "# resampled_data.to_csv(\"resampled_dataset_testing.csv\", index=False)\n",
    "#\n",
    "# # Step 6: Verify class balance\n",
    "# unique_counts = resampled_data[\"y\"].value_counts()\n",
    "# print(\"Class distribution in the resampled dataset:\")\n",
    "# print(unique_counts)\n",
    "#\n",
    "# # Step 7: Check if the counts are equal\n",
    "# if len(unique_counts) == 2 and unique_counts[0] == unique_counts[1]:\n",
    "#     print(\"Prediction column has equal count for both classes.\")\n",
    "# else:\n",
    "#     print(\"Class imbalance remains in the saved dataset.\")"
   ],
   "id": "30b0b09cfe9345ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in the resampled dataset:\n",
      "y\n",
      "0    30659\n",
      "1    30659\n",
      "Name: count, dtype: int64\n",
      "Prediction column has equal count for both classes.\n"
     ]
    }
   ],
   "execution_count": 110
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
